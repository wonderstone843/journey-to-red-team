# 🧨 Module 2 Notes: Threats to AI Systems

## 🔍 What I Learned

- **Adversarial Attacks:** Inputs designed to fool AI — like altered stop signs or poisoned training examples.
- **Prompt Injection:** Getting a language model to ignore its instructions and do something dangerous.
- **Model Extraction:** Trick the AI into revealing how it works, allowing someone to recreate or steal it.
- **Poisoning Attacks:** Tampering with training data so that the AI learns harmful or biased behavior.

## 🧠 Reflections

- These threats don’t just mess with code — they manipulate **trust.**
- The danger is often subtle: a slightly changed input, a hidden prompt, a quietly leaked parameter.
- I’ve learned how easy it is to trick a system if you understand its blind spots.

## 🚀 Takeaway

> “Adversarial inputs are like social engineering for machines.”

By understanding these threats, I’m learning how to *think like an attacker* — and how to protect against them.

📎 *File path for GitHub:* `modules/Module2_Threats.md`
