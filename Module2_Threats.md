# ğŸ§¨ Module 2 Notes: Threats to AI Systems

## ğŸ” What I Learned

- **Adversarial Attacks:** Inputs designed to fool AI â€” like altered stop signs or poisoned training examples.
- **Prompt Injection:** Getting a language model to ignore its instructions and do something dangerous.
- **Model Extraction:** Trick the AI into revealing how it works, allowing someone to recreate or steal it.
- **Poisoning Attacks:** Tampering with training data so that the AI learns harmful or biased behavior.

## ğŸ§  Reflections

- These threats donâ€™t just mess with code â€” they manipulate **trust.**
- The danger is often subtle: a slightly changed input, a hidden prompt, a quietly leaked parameter.
- Iâ€™ve learned how easy it is to trick a system if you understand its blind spots.

## ğŸš€ Takeaway

> â€œAdversarial inputs are like social engineering for machines.â€

By understanding these threats, Iâ€™m learning how to *think like an attacker* â€” and how to protect against them.

ğŸ“ *File path for GitHub:* `modules/Module2_Threats.md`
